# üîß OneMindAI System Scaling Analysis - Part 2: Solutions & Architecture

**Date:** November 20, 2025  
**Version:** 3.0.0 ‚Üí 4.0.0 (Proposed)

---

## üéØ COMPREHENSIVE SCALING SOLUTIONS

### **Phase 1: Immediate Fixes (No Backend Required)** ‚ö° 1-2 Days

#### **1.1 Prompt Input Limits & Warnings**

**Implementation:**
```typescript
// Add to OneMindAI.tsx
const LIMITS = {
  PROMPT_SOFT_LIMIT: 5000,    // Warning
  PROMPT_HARD_LIMIT: 10000,   // Block
  PROMPT_CHUNK_SIZE: 4000,    // For chunking
};

const [promptWarning, setPromptWarning] = useState<string | null>(null);

// In prompt onChange handler
const handlePromptChange = (value: string) => {
  setPrompt(value);
  
  if (value.length > LIMITS.PROMPT_HARD_LIMIT) {
    setPromptWarning(`‚ùå Prompt too long (${value.length} chars). Maximum is ${LIMITS.PROMPT_HARD_LIMIT} characters.`);
    return; // Block input
  } else if (value.length > LIMITS.PROMPT_SOFT_LIMIT) {
    setPromptWarning(`‚ö†Ô∏è Large prompt (${value.length} chars). Consider breaking into smaller requests.`);
  } else {
    setPromptWarning(null);
  }
};
```

**UI Changes:**
```tsx
{/* Add character counter */}
<div className="flex justify-between items-center text-xs text-gray-500 mt-1">
  <span>{prompt.length.toLocaleString()} / {LIMITS.PROMPT_HARD_LIMIT.toLocaleString()} characters</span>
  {promptWarning && (
    <span className={prompt.length > LIMITS.PROMPT_HARD_LIMIT ? 'text-red-600' : 'text-orange-600'}>
      {promptWarning}
    </span>
  )}
</div>
```

**Benefits:**
- ‚úÖ User sees character count in real-time
- ‚úÖ Warning before hitting limit
- ‚úÖ Prevents data loss
- ‚úÖ No backend required

---

#### **1.2 File Upload Limits & Validation**

**Implementation:**
```typescript
// Add to file-utils.ts
export const FILE_LIMITS = {
  MAX_FILE_SIZE: 10 * 1024 * 1024,      // 10 MB per file
  MAX_TOTAL_SIZE: 50 * 1024 * 1024,     // 50 MB total
  MAX_FILE_COUNT: 20,                    // 20 files max
  MAX_IMAGE_DIMENSION: 4096,             // 4096px max
  ALLOWED_TYPES: [
    'image/*',
    'text/plain',
    'application/pdf',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    'application/json',
    'text/csv',
    'application/vnd.ms-excel',
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
  ]
};

export interface FileValidationResult {
  valid: boolean;
  error?: string;
  warnings?: string[];
}

export function validateFile(file: File, currentFiles: UploadedFile[]): FileValidationResult {
  // Check file count
  if (currentFiles.length >= FILE_LIMITS.MAX_FILE_COUNT) {
    return {
      valid: false,
      error: `Maximum ${FILE_LIMITS.MAX_FILE_COUNT} files allowed. Remove some files first.`
    };
  }
  
  // Check individual file size
  if (file.size > FILE_LIMITS.MAX_FILE_SIZE) {
    return {
      valid: false,
      error: `File "${file.name}" is too large (${(file.size / 1024 / 1024).toFixed(1)} MB). Maximum is ${FILE_LIMITS.MAX_FILE_SIZE / 1024 / 1024} MB.`
    };
  }
  
  // Check total size
  const currentTotalSize = currentFiles.reduce((sum, f) => sum + f.size, 0);
  if (currentTotalSize + file.size > FILE_LIMITS.MAX_TOTAL_SIZE) {
    return {
      valid: false,
      error: `Total upload size would exceed ${FILE_LIMITS.MAX_TOTAL_SIZE / 1024 / 1024} MB limit. Remove some files first.`
    };
  }
  
  return { valid: true };
}

export async function processFilesWithValidation(
  fileList: File[], 
  currentFiles: UploadedFile[]
): Promise<{ files: UploadedFile[]; errors: string[] }> {
  const errors: string[] = [];
  const validFiles: File[] = [];
  
  for (const file of fileList) {
    const validation = validateFile(file, [...currentFiles, ...validFiles.map(f => ({
      name: f.name,
      size: f.size,
      type: f.type,
      content: '',
    }))]);
    
    if (validation.valid) {
      validFiles.push(file);
    } else {
      errors.push(validation.error!);
    }
  }
  
  const processedFiles = await processFiles(validFiles);
  return { files: processedFiles, errors };
}
```

**UI Changes:**
```tsx
// Update FileUploadZone.tsx
const [uploadErrors, setUploadErrors] = useState<string[]>([]);

const handleDrop = async (e: React.DragEvent) => {
  e.preventDefault();
  const droppedFiles = Array.from(e.dataTransfer.files);
  const { files: newFiles, errors } = await processFilesWithValidation(droppedFiles, files);
  
  if (errors.length > 0) {
    setUploadErrors(errors);
    setTimeout(() => setUploadErrors([]), 5000); // Clear after 5s
  }
  
  onFilesChange([...files, ...newFiles]);
};

// Display errors
{uploadErrors.length > 0 && (
  <div className="mt-2 p-3 bg-red-50 border border-red-200 rounded-lg">
    {uploadErrors.map((error, i) => (
      <div key={i} className="text-sm text-red-600">‚ùå {error}</div>
    ))}
  </div>
)}

// Display current usage
<div className="text-xs text-gray-500 mt-2">
  üìä {files.length} / {FILE_LIMITS.MAX_FILE_COUNT} files ‚Ä¢ 
  {(files.reduce((sum, f) => sum + f.size, 0) / 1024 / 1024).toFixed(1)} / 
  {FILE_LIMITS.MAX_TOTAL_SIZE / 1024 / 1024} MB used
</div>
```

**Benefits:**
- ‚úÖ Prevents browser crashes
- ‚úÖ Clear error messages
- ‚úÖ Usage indicators
- ‚úÖ No backend required

---

#### **1.3 Response Size Limits & Virtualization**

**Implementation:**
```typescript
// Add response size tracking
const MAX_RESPONSE_SIZE = 100000; // 100k characters

// In streaming handler
let totalChars = 0;
for await (const chunk of result.stream) {
  const text = chunk.text();
  totalChars += text.length;
  
  if (totalChars > MAX_RESPONSE_SIZE) {
    yield '\n\n‚ö†Ô∏è Response truncated at 100,000 characters to prevent browser slowdown.';
    break;
  }
  
  yield text;
}
```

**Use React Window for Long Responses:**
```bash
npm install react-window
```

```typescript
// Create VirtualizedResponse.tsx
import { FixedSizeList } from 'react-window';

export function VirtualizedResponse({ content }: { content: string }) {
  const lines = content.split('\n');
  
  return (
    <FixedSizeList
      height={600}
      itemCount={lines.length}
      itemSize={20}
      width="100%"
    >
      {({ index, style }) => (
        <div style={style}>{lines[index]}</div>
      )}
    </FixedSizeList>
  );
}
```

**Benefits:**
- ‚úÖ Prevents UI freeze
- ‚úÖ Smooth scrolling for long responses
- ‚úÖ Lower memory usage

---

#### **1.4 Request Queuing & Rate Limiting**

**Implementation:**
```typescript
// Create request-queue.ts
class RequestQueue {
  private queue: Array<() => Promise<any>> = [];
  private running = 0;
  private maxConcurrent = 3; // Max 3 concurrent requests
  
  async add<T>(fn: () => Promise<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      this.queue.push(async () => {
        try {
          const result = await fn();
          resolve(result);
        } catch (error) {
          reject(error);
        }
      });
      
      this.process();
    });
  }
  
  private async process() {
    if (this.running >= this.maxConcurrent || this.queue.length === 0) {
      return;
    }
    
    this.running++;
    const fn = this.queue.shift()!;
    
    try {
      await fn();
    } finally {
      this.running--;
      this.process();
    }
  }
}

export const requestQueue = new RequestQueue();
```

**Usage:**
```typescript
// In runAll()
const runAll = async () => {
  const selectedEngines = engines.filter(e => selected[e.id]);
  
  // Queue requests instead of running all at once
  const results = await Promise.all(
    selectedEngines.map(engine => 
      requestQueue.add(() => streamFromProvider(engine, prompt, outCap))
    )
  );
};
```

**Benefits:**
- ‚úÖ Prevents rate limit errors
- ‚úÖ Controlled concurrency
- ‚úÖ Better error handling

---

### **Phase 2: Browser Storage & Persistence** üíæ 2-3 Days

#### **2.1 IndexedDB for Conversation History**

**Implementation:**
```typescript
// Create db.ts
import { openDB, DBSchema, IDBPDatabase } from 'idb';

interface ConversationDB extends DBSchema {
  conversations: {
    key: string;
    value: {
      id: string;
      timestamp: number;
      prompt: string;
      results: RunResult[];
      files: UploadedFile[];
    };
  };
  files: {
    key: string;
    value: {
      id: string;
      name: string;
      size: number;
      type: string;
      content: string;
      timestamp: number;
    };
  };
}

class ConversationStore {
  private db: IDBPDatabase<ConversationDB> | null = null;
  
  async init() {
    this.db = await openDB<ConversationDB>('onemindai', 1, {
      upgrade(db) {
        db.createObjectStore('conversations', { keyPath: 'id' });
        db.createObjectStore('files', { keyPath: 'id' });
      },
    });
  }
  
  async saveConversation(conversation: any) {
    await this.db!.put('conversations', conversation);
  }
  
  async getConversations(limit = 50) {
    const all = await this.db!.getAll('conversations');
    return all.sort((a, b) => b.timestamp - a.timestamp).slice(0, limit);
  }
  
  async deleteOldConversations(daysOld = 30) {
    const cutoff = Date.now() - (daysOld * 24 * 60 * 60 * 1000);
    const all = await this.db!.getAll('conversations');
    
    for (const conv of all) {
      if (conv.timestamp < cutoff) {
        await this.db!.delete('conversations', conv.id);
      }
    }
  }
}

export const conversationStore = new ConversationStore();
```

**Benefits:**
- ‚úÖ Persistent storage (survives refresh)
- ‚úÖ 50 MB - 1 GB storage
- ‚úÖ Fast queries
- ‚úÖ Automatic cleanup

---

#### **2.2 LocalStorage for Settings**

**Implementation:**
```typescript
// Create settings-store.ts
interface AppSettings {
  selectedProviders: Record<string, boolean>;
  apiKeys: Record<string, string>;
  theme: 'light' | 'dark';
  maxConcurrentRequests: number;
  autoSaveConversations: boolean;
}

class SettingsStore {
  private key = 'onemindai-settings';
  
  save(settings: Partial<AppSettings>) {
    const current = this.load();
    const updated = { ...current, ...settings };
    localStorage.setItem(this.key, JSON.stringify(updated));
  }
  
  load(): AppSettings {
    const stored = localStorage.getItem(this.key);
    return stored ? JSON.parse(stored) : this.getDefaults();
  }
  
  private getDefaults(): AppSettings {
    return {
      selectedProviders: { openai: true, claude: true },
      apiKeys: {},
      theme: 'light',
      maxConcurrentRequests: 3,
      autoSaveConversations: true,
    };
  }
}

export const settingsStore = new SettingsStore();
```

**Benefits:**
- ‚úÖ Settings persist across sessions
- ‚úÖ Fast access
- ‚úÖ Simple API

---

### **Phase 3: Backend Infrastructure** üèóÔ∏è 1-2 Weeks

#### **3.1 Backend Architecture**

**Tech Stack:**
```yaml
Backend Framework: Node.js + Express / FastAPI (Python)
Database: PostgreSQL (relational data)
Cache: Redis (session, rate limiting)
Vector DB: Pinecone / Weaviate (embeddings, search)
File Storage: AWS S3 / Cloudflare R2
Queue: Bull (Redis-based job queue)
```

**Why Each Component:**

| Component | Purpose | Benefit |
|-----------|---------|---------|
| **PostgreSQL** | User accounts, conversations, metadata | ACID compliance, relations |
| **Redis** | Session cache, rate limiting, job queue | Sub-millisecond latency |
| **Vector DB** | Semantic search, RAG, embeddings | AI-powered search |
| **S3/R2** | Large file storage | Scalable, cheap storage |
| **Bull Queue** | Background jobs, retry logic | Reliable async processing |

---

#### **3.2 Backend API Design**

**Endpoints:**
```typescript
// Authentication
POST   /api/auth/register
POST   /api/auth/login
POST   /api/auth/logout
GET    /api/auth/me

// Conversations
GET    /api/conversations
GET    /api/conversations/:id
POST   /api/conversations
DELETE /api/conversations/:id
PATCH  /api/conversations/:id

// Files
POST   /api/files/upload
GET    /api/files/:id
DELETE /api/files/:id
POST   /api/files/process    // Extract text, generate embeddings

// AI Providers (Proxy)
POST   /api/ai/stream         // Unified streaming endpoint
GET    /api/ai/providers      // List available providers
POST   /api/ai/embeddings     // Generate embeddings

// Search
POST   /api/search/semantic   // Vector search
POST   /api/search/keyword    // Full-text search

// Analytics
GET    /api/analytics/usage
GET    /api/analytics/costs
```

---

#### **3.3 Database Schema**

**PostgreSQL Schema:**
```sql
-- Users table
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- API Keys table (encrypted)
CREATE TABLE api_keys (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  provider VARCHAR(50) NOT NULL,
  encrypted_key TEXT NOT NULL,
  created_at TIMESTAMP DEFAULT NOW()
);

-- Conversations table
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  title VARCHAR(500),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Messages table
CREATE TABLE messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID REFERENCES conversations(id) ON DELETE CASCADE,
  role VARCHAR(20) NOT NULL, -- 'user' or 'assistant'
  content TEXT NOT NULL,
  provider VARCHAR(50),
  tokens_used INTEGER,
  cost_usd DECIMAL(10, 6),
  created_at TIMESTAMP DEFAULT NOW()
);

-- Files table
CREATE TABLE files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  conversation_id UUID REFERENCES conversations(id) ON DELETE CASCADE,
  filename VARCHAR(500) NOT NULL,
  file_type VARCHAR(100),
  file_size BIGINT,
  storage_path TEXT NOT NULL,
  extracted_text TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);

-- Usage analytics table
CREATE TABLE usage_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  provider VARCHAR(50) NOT NULL,
  tokens_used INTEGER,
  cost_usd DECIMAL(10, 6),
  created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_conversations_user ON conversations(user_id);
CREATE INDEX idx_messages_conversation ON messages(conversation_id);
CREATE INDEX idx_files_user ON files(user_id);
CREATE INDEX idx_usage_logs_user_date ON usage_logs(user_id, created_at);
```

---

#### **3.4 Redis Cache Strategy**

**Cache Keys:**
```typescript
// Session cache
`session:${userId}` ‚Üí User session data (TTL: 24h)

// Rate limiting
`ratelimit:${userId}:${provider}` ‚Üí Request count (TTL: 1min)

// API response cache
`cache:response:${hash(prompt)}` ‚Üí Cached response (TTL: 1h)

// File processing queue
`queue:file:${fileId}` ‚Üí File processing job

// Provider health
`health:${provider}` ‚Üí Provider status (TTL: 5min)
```

**Implementation:**
```typescript
// redis-client.ts
import Redis from 'ioredis';

export const redis = new Redis({
  host: process.env.REDIS_HOST,
  port: parseInt(process.env.REDIS_PORT || '6379'),
  password: process.env.REDIS_PASSWORD,
});

// Rate limiting
export async function checkRateLimit(
  userId: string, 
  provider: string, 
  limit: number = 60
): Promise<boolean> {
  const key = `ratelimit:${userId}:${provider}`;
  const current = await redis.incr(key);
  
  if (current === 1) {
    await redis.expire(key, 60); // 1 minute window
  }
  
  return current <= limit;
}
```

---

#### **3.5 Vector Database for Semantic Search**

**Use Case:**
- Search previous conversations
- Find similar questions
- RAG (Retrieval Augmented Generation)

**Implementation (Pinecone):**
```typescript
// vector-store.ts
import { PineconeClient } from '@pinecone-database/pinecone';
import { OpenAI } from 'openai';

const pinecone = new PineconeClient();
await pinecone.init({
  apiKey: process.env.PINECONE_API_KEY!,
  environment: process.env.PINECONE_ENV!,
});

const index = pinecone.Index('onemindai-conversations');
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Generate embedding
async function generateEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  return response.data[0].embedding;
}

// Store conversation
export async function storeConversation(
  conversationId: string,
  content: string,
  metadata: any
) {
  const embedding = await generateEmbedding(content);
  
  await index.upsert([{
    id: conversationId,
    values: embedding,
    metadata: {
      ...metadata,
      content: content.substring(0, 1000), // Store preview
    },
  }]);
}

// Search similar conversations
export async function searchSimilar(query: string, topK: number = 5) {
  const embedding = await generateEmbedding(query);
  
  const results = await index.query({
    vector: embedding,
    topK,
    includeMetadata: true,
  });
  
  return results.matches;
}
```

**Benefits:**
- ‚úÖ Semantic search across conversations
- ‚úÖ Find similar questions
- ‚úÖ Context-aware responses
- ‚úÖ Duplicate detection

---

**Continue to Part 3 for Implementation Plan ‚Üí**
